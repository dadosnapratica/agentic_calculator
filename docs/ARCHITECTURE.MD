
# ğŸ—ï¸ Architecture - Agentic Calculator PoC

**Detailed technical documentation of the multi-agent architecture with local LLM**

---

> ğŸ‡§ğŸ‡· [VersÃ£o Original em PortuguÃªs (Brasil)](ARCHITECTURE_BR.md)  
> **Aviso:** Esta Ã© a versÃ£o original da documentaÃ§Ã£o tÃ©cnica. Para mÃ¡xima precisÃ£o conceitual e tÃ©cnica, utilize preferencialmente esta versÃ£o.

> ğŸ‡ºğŸ‡¸ [English Version](ARCHITECTURE.md)  
> **Disclaimer:** This documentation is also available in English. For official technical reference, consult the Portuguese or English versions.

> ğŸ‡ªğŸ‡¸ [DocumentaciÃ³n en EspaÃ±ol](ARCHITECTURE_ES.md)  
> **Aviso:** La documentaciÃ³n tambiÃ©n estÃ¡ disponible en espaÃ±ol. Para mayor precisiÃ³n tÃ©cnica, consulte la versiÃ³n original en portuguÃ©s o la versiÃ³n en inglÃ©s.

---


## ğŸ“‹ Index

- Overview
- Design Principles
- Components
- Data Flow
- Architectural Decisions
- Security
- Scalability
- Technology Stack

---

## ğŸ¯ Overview

### Layered Architecture

Presentation Layer â†’ Orchestration Layer â†’ Specialists Layer â†’ Model Layer â†’ Infrastructure Layer

Cross-cutting Layer: Security, Logging, Configuration.

---

## ğŸ§© Design Principles

### 1. Separation of Concerns
Each component has a **single responsibility**:
- Orchestrator: Coordination and flow
- LLMClient: Model communication
- Specialists: Operation execution
- Sandbox: Security validation

### 2. Open/Closed Principle
Open for extension, closed for modification.

### 3. Dependency Injection
Components receive dependencies via constructor.

### 4. Fail-Safe
The system fails gracefully and returns structured errors.

---

## ğŸ”§ Components

### CalculatorOrchestrator
Responsible for coordinating the full execution pipeline:
1. Planning (LLM)
2. Validation (Security)
3. Execution (Specialists)
4. Result consolidation

### LLMClient
Handles communication with the language model:
- Generates responses
- Converts natural language into structured operation plans

### Specialists
Execute domainâ€‘specific operations using the Strategy Pattern:
- Basic Operations
- Advanced Operations
- Statistical Operations

### Sandbox
Ensures safe execution through:
- Operation whitelist
- Execution timeout
- Argument validation
- Execution logging
- Error isolation

---

## ğŸ”„ Data Flow

Example:
User input â†’ Orchestrator â†’ LLM planning â†’ Validation â†’ Execution â†’ Consolidation â†’ UI output

Example result:
Final result returned together with execution metadata and intermediate results.

---

## ğŸ¨ Architectural Decisions

### Why Local LLM?
Chosen for:
- Privacy
- Zero operational cost
- Local experimentation capability

### Why Streamlit?
Chosen for:
- Rapid development
- Python-native workflow
- Ideal for PoC environments

### Why Multi-Agent Architecture?
Benefits:
- Modular design
- Easier testing
- Better scalability
- Natural delegation model aligned with AI agents

---

## ğŸ”’ Security

Security mechanisms include:
- Operation whitelist
- Argument validation
- Execution timeout
- Full audit logging
- Error isolation

Threats mitigated:
- Prompt injection
- Code injection
- Denial of Service
- Data exfiltration
- Privilege escalation

---

## ğŸ“ˆ Scalability

### Current (Vertical Scaling)
Single orchestrator process handling requests sequentially.

### Future (Horizontal Scaling)
- Load balancer
- Multiple orchestrator instances
- LLM pool
- Queue-based processing
- Async execution

Distributed tracing supported via OpenTelemetry.

---

## ğŸ› ï¸ Technology Stack

Backend:
- Python 3.9+
- YAML configuration
- asyncio (future)

LLM:
- Ollama
- Mistral 7B Instruct
- 8K context window

Frontend:
- Streamlit
- CLI (future)

Infrastructure:
- Ubuntu / WSL2
- Docker (future)
- Docker Compose (future)

---

## ğŸ”„ Design Patterns Used

- Strategy Pattern
- Factory Pattern
- Template Method
- Observer Pattern

---

## ğŸ“Š Metrics & Monitoring

Key metrics:
- Total latency
- LLM planning latency
- Success rate
- Error rate
- Resource usage
- Operations per minute

---

## ğŸš€ Architectural Evolution

Phase 1: PoC  
User â†’ Streamlit â†’ Orchestrator â†’ LLM â†’ Specialists

Phase 2: Production Ready  
Load Balancer + Multiple Orchestrators + Worker Pool

Phase 3: Distributed  
API Gateway + Kubernetes + Service Mesh + Message Broker

---

## ğŸ“š References

- ReAct Paper
- Chain-of-Thought Prompting
- AutoGPT Agents

Inspired by:
- LangChain
- AutoGen
- CrewAI

---

## ğŸ¤ Contributing

Changes should follow ADR (Architecture Decision Record) process.

Example:
ADR-001: Add LLM Plan Cache using Redis with TTL.

---

**Created with ğŸ—ï¸ by Flavio Lopes | Architecture v1.0 | 2026**
